{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 2 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Solutions\n",
    "\n",
    "Student Netid: Solutions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Case study\n",
    "Problem statement about \"A question asking students to walk us through the \"Target Pregnancy Prediction\" case using the framework outlined in the first class using the churn example.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Exploring data in the command line\n",
    "For this part we will be using the data file located in `\"data/advertising_events.csv\"`. This file consists of records that pertain to some advertising events on a given day. There are 4 comma separated columns in this order: `userid`, `timestamp`, `domain`, and `action`. These fields are of type `int`, `int`, `string`, and `int` respectively. Answer the following questions using Linux/Unix bash commands. All questions can be answered in one line (sometimes, with pipes)! Some questions will have many possible solutions. Don't forget that in IPython notebooks you must prefix all bash commands with an exclamation point, i.e. `\"!command arguments\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. How many records are in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10341 data/advertising_events.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/advertising_events.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. How many unique users are in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     732\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f1 -d',' data/advertising_events.csv | sort | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Rank all domains by the number of visits they received in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3114 google.com\r\n",
      "2092 facebook.com\r\n",
      "1036 youtube.com\r\n",
      "1034 yahoo.com\r\n",
      "1022 baidu.com\r\n",
      " 513 wikipedia.org\r\n",
      " 511 amazon.com\r\n",
      " 382 qq.com\r\n",
      " 321 twitter.com\r\n",
      " 316 taobao.com\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f3 -d',' data/advertising_events.csv | sort | uniq -c | sort -nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. List all records for the user with user id 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37,648061658,google.com,0\r\n",
      "37,642479972,google.com,2\r\n",
      "37,644493341,facebook.com,2\r\n",
      "37,654941318,facebook.com,1\r\n",
      "37,649979874,baidu.com,1\r\n",
      "37,653061949,yahoo.com,1\r\n",
      "37,655020469,google.com,3\r\n",
      "37,640878012,amazon.com,0\r\n",
      "37,659864136,youtube.com,1\r\n",
      "37,640361378,yahoo.com,1\r\n",
      "37,653862134,facebook.com,0\r\n",
      "37,648828970,youtube.com,0\r\n"
     ]
    }
   ],
   "source": [
    "!grep '^37,' data/advertising_events.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Dealing with messy data\n",
    "Not all data you will deal with is going to be clean. In fact, much of it will be very messy! For example, we have the HTML page that lists the contributors to Facebook's [osquery](https://github.com/facebook/osquery) project that is hosted on [Github.com](https://github.com). In this case, all we are interested in are the contributors and how many commits each of them has. Given the HTML page in `\"data/osquery_contributors.html\"` you will sift through tons of irrelevant data so that you can build a useful data structure.\n",
    "\n",
    "Notice that the first six (out of 59 total) contributors are named \"theopolis\", \"marpaia\", \"javuto\", \"jedi22\", \"unixist\", and \"mofarrell\". They have 553, 477, 104, 49, 30, 25 commits respectively.\n",
    "\n",
    "![Screenshot](images/osquery_contributors.png)\n",
    "\n",
    "To get a better of understanding of how this data is stored in the file, try searching through the raw data file for these usernames to look for any patterns. Your final dictionary should have 59 elements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Turn this data into a Python dictionary called `contributors` where the keys are the contributor names and the values are the number of commits that each contributor has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shawndavenport': '1', 'jedi22': '49', 'schettino72': '2', 'jamesgpearce': '2', 'marpaia': '477', 'wxsBSD': '20', 'blakefrantz': '6', 'yetanotherhacker': '1', 'lwhsu': '22', 'mimeframe': '3', 'd0ugal': '1', 'maus-': '9', 'kost': '1', 'polachok': '14', 'achmiel': '3', 'vmauge': '8', 'theopolis': '553', 'yannick': '1', 'stevenhilder': '1', 'SimplyAhmazing': '1', 'timzimmermann': '2', 'mark-ignacio': '1', 'mgoffin': '2', 'deniszh': '3', 'Anubisss': '2', 'vlajos': '1', 'dreid': '1', 'astanway': '6', 'arubdesu': '1', 'sharvilshah': '23', 'jreese': '2', 'justintime32': '1', 'nlsun': '3', 'mathieuk': '2', 'ecin': '1', 'blackfist': '1', 'apage43': '1', 'zwass': '14', 'mofarrell': '25', 'maclennann': '6', 'quad': '1', 'arirubinstein': '4', 'brandt': '3', 'rjeczalik': '1', 'ga2arch': '2', 'mtmcgrew': '1', 'alex': '1', 'unixist': '30', 'cdown': '4', 'javuto': '104', 'larzconwell': '1', 'castrapel': '2', 'jacknagz': '1', 'akshaydixi': '5', 'mlw': '2', 'glensc': '2', 'tburgin': '1', 'DavidGosselin': '1', 'eastebry': '9'}\n"
     ]
    }
   ],
   "source": [
    "import re # you might find this package useful\n",
    "\n",
    "contributors = dict()\n",
    "\n",
    "# Read through each line of the data\n",
    "f = open(\"data/osquery_contributors.html\", \"r\")\n",
    "for line in f:\n",
    "    # Look for the author and number of commits\n",
    "    commits_search = re.search('author=(.*)\">(.*) commit.*</a>', line)\n",
    "\n",
    "    # If we find the line, add the contributor and his counts\n",
    "    if commits_search:\n",
    "        contributors[commits_search.group(1)] = commits_search.group(2)\n",
    "            \n",
    "# This line will print your dictionary for grading purposed. Do not remove this line!!!\n",
    "print contributors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Dealing with data Pythonically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Download the data set `\"data/ads_dataset.tsv\"` and load it into a Python Pandas data frame called `ads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ads = pd.read_csv(\"data/ads_dataset.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Write a Python function called `getDfSummary()` that does the following:\n",
    "- Takes as input a data frame\n",
    "- For each variable in the data frame calculates the following features:\n",
    "  - `number_nan` to count the number of missing not-a-number values\n",
    "  - Ignoring missing, NA, and Null values:\n",
    "    - `number_distinct` to count the number of distinct values a variable can take on\n",
    "    - `mean`, `max`, `min`, `std` (standard deviation), and `25%`, `50%`, `75%` to correspond to the appropriate percentiles\n",
    "- All of these new features should be loaded in a new data frame. Each row of the data frame should be a variable from the input data frame, and the columns should be the new summary features.\n",
    "- Returns this new data frame containing all of the summary information\n",
    "\n",
    "Hint: The pandas `describe()` method returns a useful series of values that can be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getDfSummary(input_data):\n",
    "    # Get a whole bunch of stats\n",
    "    output_data = input_data.describe().transpose()\n",
    "    \n",
    "    # Count NANs\n",
    "    output_data['number_nan'] = input_data.shape[0] - output_data['count']\n",
    "    \n",
    "    # Count unique values\n",
    "    output_data['number_distinct'] = ads.apply(lambda x: len(pd.unique(x)), axis=0) \n",
    "    \n",
    "    # Remove 'count' column since it wasn't asked for\n",
    "    output_data = output_data.drop('count', 1)\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. How long does it take for your `getDfSummary()` function to work on your `ads` data frame? Show us the results below.\n",
    "\n",
    "Hint: `%timeit getDfSummary(ads)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 67.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit getDfSummary(ads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Using the results returned from `getDfSummary()`, which fields, if any, contain missing `NaN` values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy_freq\n"
     ]
    }
   ],
   "source": [
    "summary = getDfSummary(ads)\n",
    "\n",
    "for column in summary.index[summary['number_nan'] > 0]:\n",
    "    print column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. For the fields with missing values, does it look like the data is missing at random? Are there any other fields that correlate perfectly, or predict that the data is missing? If missing, what should the data value be?\n",
    "\n",
    "Hint: create another data frame that has just the records with a missing value. Get a summary of this data frame using `getDfSummary()` and compare the differences. Do some feature distributions change dramatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           mean          std       min  25%  50%         75%  \\\n",
      "isbuyer                0.042632     0.202027    0.0000    0    0    0.000000   \n",
      "buy_freq               1.240653     0.782228    1.0000    1    1    1.000000   \n",
      "visit_freq             1.852777     2.921820    0.0000    1    1    2.000000   \n",
      "buy_interval           0.210008     3.922016    0.0000    0    0    0.000000   \n",
      "sv_interval            5.825610    17.595442    0.0000    0    0    0.104167   \n",
      "expected_time_buy     -0.198040     4.997792 -181.9238    0    0    0.000000   \n",
      "expected_time_visit  -10.210786    31.879722 -187.6156    0    0    0.000000   \n",
      "last_buy              64.729335    53.476658    0.0000   18   51  105.000000   \n",
      "last_visit            64.729335    53.476658    0.0000   18   51  105.000000   \n",
      "multiple_buy           0.006357     0.079479    0.0000    0    0    0.000000   \n",
      "multiple_visit         0.277444     0.447742    0.0000    0    0    1.000000   \n",
      "uniq_urls             86.569343    61.969765   -1.0000   30   75  155.000000   \n",
      "num_checkins         720.657592  1275.727306    1.0000  127  319  802.000000   \n",
      "y_buy                  0.004635     0.067924    0.0000    0    0    0.000000   \n",
      "\n",
      "                             max  number_nan  number_distinct  \n",
      "isbuyer                  1.00000           0                2  \n",
      "buy_freq                15.00000       52257               11  \n",
      "visit_freq              84.00000           0               64  \n",
      "buy_interval           174.62500           0              295  \n",
      "sv_interval            184.91670           0             5886  \n",
      "expected_time_buy       84.28571           0              348  \n",
      "expected_time_visit     91.40192           0            15135  \n",
      "last_buy               188.00000           0              189  \n",
      "last_visit             188.00000           0              189  \n",
      "multiple_buy             1.00000           0                2  \n",
      "multiple_visit           1.00000           0                2  \n",
      "uniq_urls              206.00000           0              207  \n",
      "num_checkins         37091.00000           0             4628  \n",
      "y_buy                    1.00000           0                2  \n",
      "                           mean          std       min  25%  50%         75%  \\\n",
      "isbuyer                0.000000     0.000000    0.0000    0    0    0.000000   \n",
      "buy_freq                    NaN          NaN       NaN  NaN  NaN         NaN   \n",
      "visit_freq             1.651549     2.147955    1.0000    1    1    2.000000   \n",
      "buy_interval           0.000000     0.000000    0.0000    0    0    0.000000   \n",
      "sv_interval            5.686388    17.623555    0.0000    0    0    0.041667   \n",
      "expected_time_buy      0.000000     0.000000    0.0000    0    0    0.000000   \n",
      "expected_time_visit   -9.669298    31.239030 -187.6156    0    0    0.000000   \n",
      "last_buy              65.741317    53.484622    0.0000   19   52  106.000000   \n",
      "last_visit            65.741317    53.484622    0.0000   19   52  106.000000   \n",
      "multiple_buy           0.000000     0.000000    0.0000    0    0    0.000000   \n",
      "multiple_visit         0.255602     0.436203    0.0000    0    0    1.000000   \n",
      "uniq_urls             86.656180    61.996711   -1.0000   30   75  155.000000   \n",
      "num_checkins         721.848518  1284.504018    1.0000  126  318  803.000000   \n",
      "y_buy                  0.003024     0.054904    0.0000    0    0    0.000000   \n",
      "\n",
      "                             max  number_nan  number_distinct  \n",
      "isbuyer                  0.00000           0                2  \n",
      "buy_freq                     NaN       52257               11  \n",
      "visit_freq              84.00000           0               64  \n",
      "buy_interval             0.00000           0              295  \n",
      "sv_interval            184.91670           0             5886  \n",
      "expected_time_buy        0.00000           0              348  \n",
      "expected_time_visit     91.40192           0            15135  \n",
      "last_buy               188.00000           0              189  \n",
      "last_visit             188.00000           0              189  \n",
      "multiple_buy             0.00000           0                2  \n",
      "multiple_visit           1.00000           0                2  \n",
      "uniq_urls              206.00000           0              207  \n",
      "num_checkins         37091.00000           0             4628  \n",
      "y_buy                    1.00000           0                2  \n"
     ]
    }
   ],
   "source": [
    "ads_null = ads[ads.isnull().any(axis=1)]\n",
    "print getDfSummary(ads)\n",
    "print getDfSummary(ads_null)\n",
    "\n",
    "# buy_freq is missing when isbuyer, buy_interval, expected_time_buy, and multiple_buy are 0\n",
    "# the frequency of buying is not properly stored when no purchases are made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Which variables are binary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isbuyer\n",
      "multiple_buy\n",
      "multiple_visit\n",
      "y_buy\n"
     ]
    }
   ],
   "source": [
    "summary = getDfSummary(ads)\n",
    "\n",
    "for column in summary.index[(summary['number_distinct'] == 2) & (summary['min'] == 0) & (summary['max'] == 1)]:\n",
    "    print column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
