{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: \n",
    "\n",
    "Student Netid:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Critique this plan\n",
    "1\\. After a few beers your CIO invited his buddy from Blue Moon consulting to propose a project using data mining to improve the targeting of the new service that you have been a principal in developing. The service has been quite successful so far, being marketed over the last 6 months via your ingenious, and very inexpensive, word-of-mouth campaign. You've already garnered a pretty large customer base without any targeting, and you've been seeing this success as your best stepping stone to bigger and better things in the firm. \n",
    "\n",
    "After some reflection, you've decided that your best course of action is to play a key role in ensuring the success of this data mining project as well. You agree with your CIO's statement in a meeting with Blue Moon, that accurate targeting might cost-effectively expand your audience substantially to consumers that word-of-mouth would not reach. You accept that what Blue Moon says about the characteristics of your service is accurate.\n",
    "\n",
    "Based on what we have covered in class and in the book, identify the four most serious weaknesses/flaws in this abridged version of Blue Moon's proposal, and suggest how to ameliorate them.  Your answer should be 4 bullet points, each comprising 2-4 sentences: 1-2 sentences stating each weakness, and 1-2 sentences suggesting a better alternative.  Maximal credit will be given when the 4 points are as independent as possible.\n",
    "\n",
    "\n",
    "```\n",
    "--- -------------------------------------------------------------------------- ---\n",
    "                            Targeted Audience Expansion             \n",
    "                      Prepared by Blue Moon Consulting, Inc.\n",
    "\n",
    "Your problem is to expand the audience of your new service.  We (Blue Moon) have a \n",
    "large database of consumers who can be targeted.  We will build a predictive model \n",
    "to estimate which of these consumers are the most likely to adopt the product, and\n",
    "then target them with the special offer you have designed.\n",
    "\n",
    "More specifically, we will build a logistic regression (LR) model to predict adop-\n",
    "tion of the service by a consumer, based on the data on your current customers of \n",
    "this service.  The model will be based on their demographics and their usage of \n",
    "the service. We believe that logistic regression is the best choice of method be-\n",
    "cause it is a tried-and-true statistical modeling technique, and we can easily \n",
    "interpret the coefficients of the model to infer whether the attributes are stat-\n",
    "istically significant, and whether they make sense. If they are statistically sig-\n",
    "nificant and they do make sense, then we can have confidence that the model will \n",
    "be accurate in predicting service uptake. We will apply the model to our large \n",
    "database of consumers, and select out those who have not yet subscribed and whom\n",
    "the LR model predicts to be the most likely to subscribe.  To these we will send \n",
    "the targeted offer. As this is a fixed-profit-per-customer service, this also \n",
    "will in effect rank them by expected profit.\n",
    "--- -------------------------------------------------------------------------- ---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. From your reading you know that the naive Bayes classifier works by calculating the conditional probabilities of each feature, $e_i$, occuring with each class $c$ and treating them independently. This results in the probability of a certain class occuring given a set of features, or a piece of evidence, $E$, as\n",
    "\n",
    "$$P(c \\mid E) = \\frac{p(e_1 \\mid c) \\cdot p(e_2 \\mid c) \\cdot \\cdot \\cdot p(e_k \\mid c) \\cdot p(c)}{p(E)}.$$\n",
    "\n",
    "The conditional probability of each piece of evidence occuring with a given class is given by\n",
    "\n",
    "$$P(e_i \\mid c) = \\frac{\\text{count}(e_i, c)}{\\text{count}(c)}.$$\n",
    "\n",
    "In the above equation $\\text{count}(e_i, c)$ is the number of documents in a given class that contain feature $e_i$ and $\\text{count}(c)$ is the number of documents that belong to class $c$. \n",
    "\n",
    "A common variation of the above is to use Laplace (sometimes called +1) smoothing. Recall the use of Laplace smoothing introduced toward the end of Chapter 3 in the section Probability Estimation. This is done in sklearn by setting `alpha=1` in the `BernoulliNB()` function (this is also the default behavior). The result of Laplace smoothing will slightly change the conditional probabilities,\n",
    "\n",
    "$$P(e_i \\mid c) = \\frac{\\text{count}(e_i, c) + 1}{\\text{count}(c) + 2}.$$\n",
    "\n",
    "In no more than **one paragraph**, describe why this is useful. Try to think of a case when not using Laplace smoothing would result in \"bad\" models. Try to give an example. Be precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Text classification for sentiment analysis\n",
    "For this part of the assignment, we are going to use a data set of movie ratings from IMDB.com. The data consists of the text of a movie review and a target variable which tells us whether the reviewer had a positive feeling towards the movie (equivalent to rating the movie between 7 and 10) or a negative feeling (rating the movie between 1 and 4). Neutral reactions are not included in the data.\n",
    "\n",
    "The data are located in \"`data/imdb.csv`\". The first column is the review text; the second is the text label 'P' for positive or 'N' for negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Load the data into a pandas `DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Code the target variable to be numeric: use the value `1` to represent 'P' and `0` to represent 'N'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Put all of the text into a data frame called `X` and the target variable in a data frame called `Y`. Make a train/test split where you give 75% of the data to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Create a binary `CountVectorizer()` and `TfidfVectorizer()`. Use the original single words as well as bigrams. Also, use an \"english\" stop word list. Fit these to the training data to extract a vocabulary and then transform both the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "binary_vectorizer = None\n",
    "X_train_binary = None\n",
    "X_test_binary = None\n",
    "\n",
    "tfidf_vectorizer = None\n",
    "X_train_tfidf = None\n",
    "X_test_tfidf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Create `LogisticRegression()` and `BernoulliNB()` models. For all settings, keep the default values. In a single plot, show the AUC curve for both classifiers and both the binary and tfidf feature sets. In the legend, include the area under the ROC curve (AUC). Do not forget to label your axes. Your final plot will be a single window with 4 curves.\n",
    "\n",
    "Which model do you think does a better job? Why? Explain in no more than a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this so your plots show properly\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5\\. Use the model from question 4 that you think did the best job and predict the rating of the test data. Find 5 examples the should have been positive, but were incorrectly classified as negative. List the text below and include an explanation as to why you think it may have been incorrectly classified. You can pick any 5. They do not have to be at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here to display 5 incorrect reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation for the 5 reviews chosen here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
