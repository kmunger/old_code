{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: FP2\n",
    "\n",
    "Student Netid: \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "Mutual Information, Entropy, Conditional Entropy, and Information Gain are terms that one encounters frequently in data science discussions.  These quantities are closely related.  Given discrete random variables $X$ and $Y$: \n",
    "\n",
    "$$\\text{Mutual Information} = \\sum_{y \\in Y} \\sum_{x \\in X} p(x, y) \\cdot log\\frac{p(x, y)}{p(x)p(y)}$$\n",
    "\n",
    "$$\\text{Entropy} = H(Y) = -\\sum_{y \\in Y} p(y) \\cdot log(p(y))$$\n",
    "\n",
    "$$\\text{Conditional Entropy} = H(Y \\mid X) = \\sum_{x \\in X} p(x) \\cdot H(Y \\mid X = x)$$\n",
    "\n",
    "Your task: show mathematically that $\\text{Mutual Information} = \\text{Information Gain}$, where $\\text{Information Gain} = H(Y) â€“ H(Y \\mid X)$. Give the derivation below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place your answer here! You can type in math by using $\\LaTeX$ (see the question for some examples). You can also hand write this, scan it in, and insert it as an image into this notebook. Try $\\LaTeX$!  \n",
    "\n",
    "NB: data science documents often are written in $\\LaTeX$, because it is especially convenient for typesetting math formulas.  Here is one beginner's guide -- skip to Section 7 for the math part. [Google: introduction latex princeton ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "1\\. It is essential to be able to differentiate between the two analytics modes: data mining and use of the results of data mining.  Label each case as describing either data mining `(DM)`, or the use of the results of data mining `(USE)`.  [Replace `(ANS)` below.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) `(ANS)` Choose customers who are most likely to respond to an on-line ad.\n",
    "\n",
    "b) `(ANS)` Discover rules that indicate when an account has been defrauded.\n",
    "\n",
    "c) `(ANS)` Find patterns indicating what customer behavior is more likely to lead to response to an on-line ad.\n",
    "\n",
    "d) `(ANS)` Estimate probability of default for a credit application.\n",
    "\n",
    "e) `(ANS)` Predict whether a customer is pregnant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Plumbing Inc. has been selling plumbing supplies for the last 20 years. The owner, Joe, decides that next year it is finally time to diversify by adding gardening tools to his products. Having had success using customer data to build predictive models to guide direct mail campaigns for special plumbing offers, he considers that data mining could help him to identify a subset of customers who should be good prospects for his new set of products. Is Joe ready to solve this as a supervised learning problem? What would you suggest as the target variable?  Be precise. Is there anything else that you would recommend that Joe do to achieve his business goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pleace your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "This is a hands-on task where we will build a tree-structured predictive model as discussed in class and in the book. For this part, we will be using the data in `data/cell2cell_data_80_percent.csv`.\n",
    "\n",
    "These historical data consist of 31,892 customers: 15,855 customers that churned (i.e., left the company) and 16,036 that did not churn (see the `\"churndep\"` variable). Here are the data set's 11 attributes describing the customers: \n",
    "\n",
    "```\n",
    "Pos.  Var. Name  Var. Description\n",
    "----- ---------- --------------------------------------------------------------\n",
    "1     revenue    Mean monthly revenue in dollars\n",
    "2     outcalls   Mean number of outbound voice calls\n",
    "3     incalls    Mean number of inbound voice calls\n",
    "4     months     Months in Service\n",
    "5     eqpdays    Number of days the customer has had his/her current equipment\n",
    "6     webcap     Handset is web capable\n",
    "7     marryyes   Married (1=Yes; 0=No)\n",
    "8     travel     Has traveled to non-US country (1=Yes; 0=No)\n",
    "9     pcown      Owns a personal computer (1=Yes; 0=No)\n",
    "10    creditcd   Possesses a credit card (1=Yes; 0=No)\n",
    "11    retcalls   Number of calls previously made to retention team\n",
    "```\n",
    "\n",
    "The 12th column, the target variable `\"churndep\"`, equals 1 if the customer churned, and 0 otherwise. \n",
    "\n",
    "**`VVV` VERY IMPORTANT NOTE `VVV`**\n",
    "\n",
    "**Don't forget to exclude the target variable `\"churndep\"` when fitting your models. You don't want to include the target when fitting!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Load the data into a pandas `DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Any code here!\n",
    "\n",
    "data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Using the following two functions for Entropy and Information Gain (don't forget to run this cell!), figure out what is the maximum information gain for each feature. Make a bar plot with feature names along the x-axis and maximum information gain on the y-axis. Which one is the largest? Don't forget some of the features are binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def entropy(target):\n",
    "    # Get the number of users\n",
    "    n = len(target)\n",
    "    # Count how frequently each unique value occurs\n",
    "    counts = np.bincount(target).astype(float)\n",
    "    # Initialize entropy\n",
    "    entropy = 0\n",
    "    # If the split is perfect, return 0\n",
    "    if len(counts) <= 1 or 0 in counts:\n",
    "        return entropy\n",
    "    # Otherwise, for each possible value, update entropy\n",
    "    for count in counts:\n",
    "        entropy += math.log(count/n, len(counts)) * count/n\n",
    "    # Return entropy\n",
    "    return -1 * entropy\n",
    "\n",
    "def information_gain(feature, threshold, target):\n",
    "    '''\n",
    "    This function takes three things:\n",
    "    feature - A list of all the possible values this feature has, e.g. data['revenue']\n",
    "    threshold - A number to threshold a continuous variable on, e.g. 1.2\n",
    "    target - A list of all the target values in the same order as feature, e.g. data['churndep']\n",
    "    '''\n",
    "    # Dealing with numpy arrays makes this slightly easier\n",
    "    target = np.array(target)\n",
    "    feature = np.array(feature)\n",
    "    # Record if the feature vector is above the threshold\n",
    "    feature = (feature <= threshold)\n",
    "    # Initialize information gain with the parent entropy\n",
    "    ig = entropy(target)\n",
    "    # For both sides of the threshold, update information gain\n",
    "    for level, count in zip([0, 1], np.bincount(feature).astype(float)):\n",
    "        ig -= count/len(feature) * entropy(target[feature == level])\n",
    "    # Return information gain\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Now build and fit a tree-structured model using `DecisionTreeClassifier()` [(manual page)](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using the 11 attributes to predict the `\"churndep\"` target variable. Make sure to use `criterion='entropy'` when instantiating an instance of `DecisionTreeClassifier()`. For all other settings you should use the default options (this means you don't have to set anything).\n",
    "\n",
    "**Remember, don't forget to exclude the target variable `\"churndep\"` when fitting your models. You don't want to fit on the target!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Load in another data set `data/cell2cell_data_20_percent.csv`. This data is of the same format as the other file we read in. Using the classifier built and fit in 3.3, predict `\"churndep\"` on the original data and the new data that you just loaded in. How well does it predict? (I.e., what is the accuracy on both data sets?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "data_new = None\n",
    "\n",
    "# Code here!!!\n",
    "\n",
    "tree_original_accuracy = None\n",
    "tree_new_accuracy = None\n",
    "\n",
    "# This lines will be used for grading. DO NOT REMOVE IT. Make sure it prints out the correct value!!!\n",
    "print \"Original and new tree accuracy = %.4f and %.4f\" % (tree_original_accuracy, tree_new_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "The default options for your decision tree may not be optimal. We need to analyze whether tuning the parameters can improve the accuracy of the classifier.  For the following options `max_depth`, `min_samples_split`, and `min_samples_leaf`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Generate a range of 10 values of each that make sense to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "min_samples_split_values = None\n",
    "min_samples_leaf_values = None\n",
    "max_depth_values = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. For the values of `max_depth`, `min_samples_split`, and `min_samples_leaf` you chose in 4.1 build a new decision tree classifier (on the original data we read in, the variable `data`) and record the classifier's accuracy on both the original data (the variable `data`) and the new data we read in (the variable `data_new`). You should now generate three plots, each with 10 points for the original data and 10 points for the new data. The values you chose are on the x-axis, the accuracies you calculated are on the y-axis, and there will be two lines/curves per plot (one for `data` and the other for `data_new`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for value in min_samples_split_values:\n",
    "    # Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for value in min_samples_leaf_values:\n",
    "    # Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for value in max_depth_values:\n",
    "    # Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Now that you have read Chapter 4 of your textbook, let's try fitting some linear models: a logistic regression (`sklearn.linear_model.LogisticRegression()`, [manual](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) and SVM (`sklearn.svm.LinearSVC()`, [manual](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)). For each of these models, fit them on the first set of data we read in (the variable `data`) and report the accuracy on both sets of data we read in (`data` and `data_new`). When fitting each model, you should keep all parameters as their defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Code here\n",
    "\n",
    "logistic_regression_original_accuracy = None\n",
    "logistic_regression_new_accuracy = None\n",
    "svm_original_accuracy = None\n",
    "svm_new_accuracy = None\n",
    "\n",
    "# These lines will be used for grading. DO NOT REMOVE THEM. Make sure they print out the correct values!!!\n",
    "print \"Original and new logistic regression accuracy = %.4f and %.4f\" % (logistic_regression_original_accuracy, logistic_regression_new_accuracy)\n",
    "print \"Original and new SVM accuracy = %.4f and %.4f\" % (svm_original_accuracy, svm_new_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
