{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Similarity, Neighbors & Clustering\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial import distance\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "# I've abstracted some of what we'll be doing today into a library.\n",
    "# You can take a look at this code if you want by going into `dstools/data_tools.py`\n",
    "from dstools import data_tools\n",
    "\n",
    "np.random.seed(36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We have a data set describing many characteristics of scotch. You can find it in `data/scotch.csv`.\n",
    "\n",
    "The data consists of 5 general whiskey attributes, each of which has many possible values:\n",
    "\n",
    "- **Color**: yellow, very pale, pale, pale gold, gold, old gold, full gold, amber, etc.\n",
    "- **Nose**: aromatic, peaty, sweet, light, fresh, dry, grassy, etc.\n",
    "- **Body**: soft, medium, full, round, smooth, light, firm, oily.\n",
    "- **Palate**: full, dry, sherry, big, fruity, grassy, smoky, salty, etc.\n",
    "- **Finish**: full, dry, warm, light, smooth, clean, fruity, grassy, smoky, etc.\n",
    "\n",
    "Let's read it in and take a look. There are a few other features unrelated to the ones above. For this class, we will be dropping them. However, feel free to check them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/scotch.csv\")\n",
    "data = data.drop([u'age', u'dist', u'score', u'percent', u'region', u'district', u'islay', u'midland', u'spey', u'east', u'west', u'north ', u'lowland', u'campbell', u'islands'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Similarity measures\n",
    "Each of our whiskeys can be described by its feature vector (68 attributes). Consider Foster's favorite, \"Bunnahabhain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data_tools.feature_printer(data, 'Bunnahabhain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other entries do we have that are similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a full list of the distance metrics supported by scipy, check:\n",
    "# http://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "\n",
    "def whiskey_distance(name, distance_measures, n):\n",
    "    # We want a data frame to store the output\n",
    "    distances = pd.DataFrame()\n",
    "    \n",
    "    # Find the location of the whiskey we are looking for\n",
    "    whiskey_location = np.where(data.index == name)[0][0]\n",
    "\n",
    "    # Go through all distance measures we care about\n",
    "    for distance_measure in distance_measures:\n",
    "        # Find all pairwise distances\n",
    "        current_distances = distance.squareform(distance.pdist(data, distance_measure))\n",
    "        # Get the closest n for the whiskey we care about\n",
    "        most_similar = np.argsort(current_distances[:, whiskey_location])[0:n]\n",
    "        # Append results\n",
    "        distances[distance_measure] = zip(data.index[most_similar], current_distances[most_similar, whiskey_location])\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whiskey_distance('Bunnahabhain', ['euclidean'], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whiskey_distance('Bunnahabhain', ['euclidean', 'cityblock', 'cosine'], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the features some of these have and see why they are ranked as being most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data_tools.feature_printer(data, 'Bunnahabhain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data_tools.feature_printer(data, 'Bruichladdich')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data_tools.feature_printer(data, 'Ardberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "Can we find groups of whiskeys that are simliar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical\n",
    "\n",
    "Given a set of records (A-F) with two features, we can visualize them on a 2 dimensional surface. Here, circles are drawn around groups of points that are the most \"similar\" to one another (closest together). These nested circles form a hierarchy.\n",
    "\n",
    "<img src=\"images/cutting.png\" height=40% width=40%>\n",
    "\n",
    "Consider our whiskey data. Here we can visualize a part of this hierarchy in a dendrogram.\n",
    "\n",
    "<img src=\"images/cross_section.png\" height=70% width=70%>\n",
    "\n",
    "Let's see what it looks like for all of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function gets pairwise distances between observations in n-dimensional space.\n",
    "dists = pdist(data, metric=\"cosine\")\n",
    "\n",
    "# This function performs hierarchical/agglomerative clustering on the condensed distance matrix y.\n",
    "links = linkage(dists)\n",
    "\n",
    "# Now we want to plot the dendrogram\n",
    "plt.rcParams['figure.figsize'] = 32, 16\n",
    "den = dendrogram(links)\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Distance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "plt.rcParams['figure.figsize'] = 10, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to cut dendrograms at a particular height and to then use the resulting clusters. For example, here we have ten clusters labeled by the \"best known\" whiskey in the group.\n",
    "\n",
    "<img src=\"images/clustering.png\" height=90% width=90%>\n",
    "\n",
    "[That's David Whishart who wrote a well-known book based on clustering whiskeys.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans\n",
    "\n",
    "Another method for finding clusters is to use the KMeans algorithm to find a set of $k$ clusters. Here, unlike in hierarchical clustering, we define the number of clusters in advance.\n",
    "\n",
    "Here is a nice illustrated example: http://util.io/k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_clusters = 6\n",
    "\n",
    "# Fit and predict clusters\n",
    "model = KMeans(k_clusters)\n",
    "model.fit(data)\n",
    "clusters = model.predict(data)\n",
    "\n",
    "# Do some messy stuff to print a nice table of clusters\n",
    "cluster_listing = {}\n",
    "for cluster in range(k_clusters):\n",
    "    cluster_listing['Cluster ' + str(cluster)] = [''] * 109\n",
    "    where_in_cluster = np.where(clusters == cluster)[0]\n",
    "    cluster_listing['Cluster ' + str(cluster)][0:len(where_in_cluster)] = data.index[where_in_cluster]\n",
    "\n",
    "# Print clusters\n",
    "pd.DataFrame(cluster_listing).loc[0:np.max(np.bincount(clusters)) - 1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we name or describe these clusters? \n",
    "\n",
    "[That's a question for you!]\n",
    "\n",
    "Let's take a look at the results of a particular clustering from Lapointe and Legendre's *A Classification of Pure Malt Scotch Whiskies*. In this clustering, they create 12 clusters A through L. Let's take cluster J as an example and built a decision tree that will classifier all whiskies as either belonging to J or not belonging to J.\n",
    "\n",
    "<img src=\"images/cluster_tree.png\" height=50% width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok.\n",
    "\n",
    "Now let's take a look at a different data set that only has two features. This will make visualizing the data much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = data_tools.make_cluster_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the data and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:, 1], s=20)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did above, we can apply KMeans to this data. Let's try a few different values for the number of clusters $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KMeans\n",
    "model = KMeans(2)\n",
    "model.fit(X)\n",
    "clusters = model.predict(X)\n",
    "plt.scatter(X[:,0], X[:, 1], color=data_tools.colorizer(clusters), linewidth=0, s=20)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "What if we have labels for our records? In this 2 dimensional data that we're using, we have five numerical labels, 0 through 4. One way to use clustering to build a classifier is to use the K-Nearest Neighbors algorithm.\n",
    "\n",
    "We'll start by splitting our X and Y data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try setting the number of neighbors to use, $k$, to a few different values and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "model = KNeighborsClassifier(10)\n",
    "model.fit(X_train, Y_train)\n",
    "data_tools.Decision_Surface(X, Y, model, cell_size=.05, surface=True, points=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that as we make $k$ smaller, we get many smaller blobs all bunched together. What happens when we get down to $k=1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have 5 classes, we can still use the evaluation metrics we have already learned about. Accuracy should be straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in [1, 10, 100]:\n",
    "    model = KNeighborsClassifier(k)\n",
    "    model.fit(X_train, Y_train)\n",
    "    print \"Accuracy with k = %d is %.3f\" % (k, metrics.accuracy_score(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few small tweaks, we can also look at AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in [1, 10, 100]:\n",
    "    model = KNeighborsClassifier(k)\n",
    "    model.fit(X_train, Y_train)\n",
    "    probabilities = model.predict_proba(X_test)\n",
    "\n",
    "    print \"KNN with k = %d\" % k\n",
    "    aucs = 0\n",
    "    for i in range(5):\n",
    "        auc = metrics.roc_auc_score(Y_test == i, probabilities[:,i])\n",
    "        aucs += auc\n",
    "        print \"   AUC for label %d vs. rest = %.3f\" % (i, auc)\n",
    "    print \"   Average AUC = %.3f\\n\" % (aucs / 5.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
