{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Fitting mathematical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# I've abstracted some of what we'll be doing today into a library.\n",
    "# You can take a look at this code if you want by going into `dstools/data_tools.py`\n",
    "from dstools import data_tools\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 14, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get some data\n",
    "target_name, variable_names, data, Y = data_tools.create_data()\n",
    "\n",
    "# Grab the predictors\n",
    "X = data_tools.X()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data. We have two features, `humor` and `number_pets`. We will use these to predict whether or not our users are `success`ful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_tools.Decision_Surface(X, Y, None, surface=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifiers\n",
    "We can re-explore the modeling technique we learned last class -- decision tree classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "depths = [1] # We can also try 7 and 20\n",
    "show_probabilities = False\n",
    "\n",
    "position = 1\n",
    "for depth in depths:\n",
    "    # Get complex x\n",
    "    X = data_tools.X()\n",
    "\n",
    "    # Model\n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    model.fit(X, Y) \n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(1, len(depths), position)\n",
    "    position += 1\n",
    "    data_tools.Decision_Surface(X, Y, model, probabilities=show_probabilities)\n",
    "    plt.title(\"Decision Tree Classifier (max depth=\" + str(depth) + \")\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you can use different max depths to create more complex models. Try it out above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear discriminant models\n",
    "\n",
    "Since you've read about linear models now, let's try building one on this data set. Looking at the data above, can you tell where a good linear discriminant would be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "show_probabilities = False\n",
    "\n",
    "# Model\n",
    "X = data_tools.X()\n",
    "model = LogisticRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Plot\n",
    "data_tools.Decision_Surface(X, Y, model, probabilities=show_probabilities)\n",
    "plt.title(\"Linear model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Ok.  We've talked a lot about the need to estimate probabilities.  Last time we discussed how tree models generate probabilities.  How would we generate probabilities from a linear discriminant?  [Let's discuss the derivation of logistic regression -- it's important to understand that.]\n",
    "\n",
    "Try showing probabilities both for the linear model and the tree classifiers. You can do this by modifying the settings at the top of each code block above (`show_probabilities = True` or `False`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear models\n",
    "\n",
    "We saw that the tree models could get very complex to fit the data better. One way of doing this was to allow the tree to grow deeper. Deeper trees resulted in cutting the decision surface into smaller and smaller pieces.\n",
    "\n",
    "Can we use the idea of fitting linear models to generate non-linear boundaries with logistic regression? We can do this by adding in some non linear terms, such as $humor^2$ or $humor^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "max_order = 3 # Maximum of 3\n",
    "show_probabilities = False\n",
    "\n",
    "for order in range(1, max_order+1):\n",
    "    # Get complex x\n",
    "    X_complex = data_tools.X(order)\n",
    "    \n",
    "    # Model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_complex, Y)\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(1, max_order, order)\n",
    "    data_tools.Decision_Surface(X_complex, Y, model, probabilities=show_probabilities)\n",
    "    plt.title(\"Linear model \" + str(order) + \"-order\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also look at the probabilities on these surfaces. Try it out above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function fitting, more generally\n",
    "\n",
    "Let's take a step back and talk more generally about fitting functions to data...  [Class discussion.]\n",
    "\n",
    "But...  we're still \"just\" fitting the *training* data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "\n",
    "What we want are models that **generalize** to data that were not used to build them! Do we even know how well our models generalize? Why is this important?\n",
    "\n",
    "Let's apply this concept to our data. Now, before we fit out models, we can set aside some data to be used later for testing. The benefit of holding out data is that our model can't simply memorize everything! Let's use sklearn to set aside some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set randomness so that we all get the same answer\n",
    "np.random.seed(841)\n",
    "\n",
    "# Split the data into train and test pieces for both X and Y\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our data, let's revisit the tree based classifier we discussed in our last class. We can start by checking how well a model does when it is fit on a \"training\" set and then used to predict on both the training set as well as our hold out set. Remember, the model has never seen this hold out \"test\" set before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print \"Accuracy on training = %.3f\" % metrics.accuracy_score(model.predict(X_train), Y_train)\n",
    "print \"Accuracy on test = %.3f\" % metrics.accuracy_score(model.predict(X_test), Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the test set were worse. Why is this? Can it ever do beter?\n",
    "\n",
    "If you remember, last week we saw that as we let our tree get deeper and deeper, we eventually achieved 100% accuracy. Can we do this on the test data? What happens as our tree gets more and more complicated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "depths = range(1, 21)\n",
    "\n",
    "for md in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=md)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    accuracies_train.append(metrics.accuracy_score(model.predict(X_train), Y_train))\n",
    "    accuracies_test.append(metrics.accuracy_score(model.predict(X_test), Y_test))\n",
    "\n",
    "plt.plot(depths, accuracies_train, label=\"Train\")\n",
    "plt.plot(depths, accuracies_test, label=\"Test\")\n",
    "plt.title(\"Performance on train and test data\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0.80, 1.0])\n",
    "plt.xlim([1,20])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "Above, we made a single train/test split. We set aside 20% of our data and *never* used it for training. We also never used the 80% of the data set aside for training to test generalizability. How can we make better use of our data so that we can utilize it more fully when training and testing?\n",
    "\n",
    "Instead of only making the split once, let's use cross validation so that every record can contribute to training as well as testing in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print \"Cross validation accuracy on training = %.3f\" % np.mean(cross_validation.cross_val_score(model, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this to our plot from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracies_cross_validation = []\n",
    "depths = range(1, 21)\n",
    "\n",
    "for md in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=md)\n",
    "    \n",
    "    accuracies_cross_validation.append(np.mean(cross_validation.cross_val_score(model, X, Y)))\n",
    "\n",
    "plt.plot(depths, accuracies_cross_validation, label=\"Cross Validation\")\n",
    "plt.plot(depths, accuracies_train, label=\"Train\")\n",
    "plt.plot(depths, accuracies_test, label=\"Test\")\n",
    "plt.title(\"Performace on train and test data\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.xlim([1,20])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity control\n",
    "\n",
    "What we did above was tune a \"hyper-parameter\" (in this case we were looking at `max_depth`, although you could also look at `min_samples_leaf` and `min_samples_split`) of the tree classifier model. \n",
    "\n",
    "What could we tune to control complexity for linear modeling like logistic regression? Next week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
